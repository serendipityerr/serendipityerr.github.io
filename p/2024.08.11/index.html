<!DOCTYPE html>
<html lang="en-us" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="Learning Diffusers\rInstallation\rWith pip\npip install --upgrade diffusers[torch] With conda\nconda install -c conda-forge diffusers Use\rDirectly call the pretrained model uploaded in diffusers:\nimport torch from diffusers import DDPMPipeline device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;) # Load the butterfly pipeline butterfly_pipeline = DDPMPipeline.from_pretrained( &#34;johnowhitaker/ddpm-butterflies-32px&#34; ).to(device) # Create 8 images images = butterfly_pipeline(batch_size=8).images # View the result make_grid(images) Example\rStep 0: Login and Initialize some useful functions\r# Login from huggingface_hub import notebook_login notebook_login() # copy the token in import numpy as np import torch import torch.nn.functional as F from matplotlib import pyplot as plt from PIL import Image def show_images(x): &#34;&#34;&#34;Given a batch of images x, make a grid and convert to PIL&#34;&#34;&#34; x = x * 0.5 + 0.5 # Map from (-1, 1) back to (0, 1) grid = torchvision.utils.make_grid(x) grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255 grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8)) return grid_im Step 1: Download a training dataset\rFor this example, we&rsquo;ll use a dataset of images from the Hugging Face Hub. Specifically, this collection of 1000 butterfly pictures.\n">
<title>2024.08.11</title>

<link rel='canonical' href='https://example.com/p/2024.08.11/'>

<link rel="stylesheet" href="/scss/style.min.b9c8156d464c343bdacaf14a871581fb94cbbdb9dd5cbce4ba017361187cc930.css"><meta property='og:title' content="2024.08.11">
<meta property='og:description' content="Learning Diffusers\rInstallation\rWith pip\npip install --upgrade diffusers[torch] With conda\nconda install -c conda-forge diffusers Use\rDirectly call the pretrained model uploaded in diffusers:\nimport torch from diffusers import DDPMPipeline device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;) # Load the butterfly pipeline butterfly_pipeline = DDPMPipeline.from_pretrained( &#34;johnowhitaker/ddpm-butterflies-32px&#34; ).to(device) # Create 8 images images = butterfly_pipeline(batch_size=8).images # View the result make_grid(images) Example\rStep 0: Login and Initialize some useful functions\r# Login from huggingface_hub import notebook_login notebook_login() # copy the token in import numpy as np import torch import torch.nn.functional as F from matplotlib import pyplot as plt from PIL import Image def show_images(x): &#34;&#34;&#34;Given a batch of images x, make a grid and convert to PIL&#34;&#34;&#34; x = x * 0.5 + 0.5 # Map from (-1, 1) back to (0, 1) grid = torchvision.utils.make_grid(x) grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255 grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8)) return grid_im Step 1: Download a training dataset\rFor this example, we&rsquo;ll use a dataset of images from the Hugging Face Hub. Specifically, this collection of 1000 butterfly pictures.\n">
<meta property='og:url' content='https://example.com/p/2024.08.11/'>
<meta property='og:site_name' content='Serendipity_Blog'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:published_time' content='2024-08-11T12:09:12&#43;08:00'/><meta property='article:modified_time' content='2024-08-11T12:09:12&#43;08:00'/>
<meta name="twitter:title" content="2024.08.11">
<meta name="twitter:description" content="Learning Diffusers\rInstallation\rWith pip\npip install --upgrade diffusers[torch] With conda\nconda install -c conda-forge diffusers Use\rDirectly call the pretrained model uploaded in diffusers:\nimport torch from diffusers import DDPMPipeline device = torch.device(&#34;cuda&#34; if torch.cuda.is_available() else &#34;cpu&#34;) # Load the butterfly pipeline butterfly_pipeline = DDPMPipeline.from_pretrained( &#34;johnowhitaker/ddpm-butterflies-32px&#34; ).to(device) # Create 8 images images = butterfly_pipeline(batch_size=8).images # View the result make_grid(images) Example\rStep 0: Login and Initialize some useful functions\r# Login from huggingface_hub import notebook_login notebook_login() # copy the token in import numpy as np import torch import torch.nn.functional as F from matplotlib import pyplot as plt from PIL import Image def show_images(x): &#34;&#34;&#34;Given a batch of images x, make a grid and convert to PIL&#34;&#34;&#34; x = x * 0.5 + 0.5 # Map from (-1, 1) back to (0, 1) grid = torchvision.utils.make_grid(x) grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255 grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8)) return grid_im Step 1: Download a training dataset\rFor this example, we&rsquo;ll use a dataset of images from the Hugging Face Hub. Specifically, this collection of 1000 butterfly pictures.\n">
    <link rel="shortcut icon" href="/favicon.ico" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column compact"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="Toggle Menu">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu_f509edb42ecc0ebd.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">ðŸ©·</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">Serendipity_Blog</a></h1>
            <h2 class="site-description">Lorem ipsum dolor sit amet, consectetur adipiscing elit.</h2>
        </div>
    </header><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/links/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>Links</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
                    <li id="dark-mode-toggle">
                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="8" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="16" cy="12" r="2" />
  <rect x="2" y="6" width="20" height="12" rx="6" />
</svg>



                        <span>Dark Mode</span>
                    </li>
                
            </ol>
        </li>
    </ol>
</aside>

    

            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/2024.08.11/">2024.08.11</a>
        </h2>
    
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Aug 11, 2024</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    5 minute read
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <h3 id="learning-diffusers">Learning <code>Diffusers</code>
</h3><h4 id="installation">Installation
</h4><p>With <code>pip</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install --upgrade diffusers<span style="color:#f92672">[</span>torch<span style="color:#f92672">]</span>
</span></span></code></pre></div><p>With <code>conda</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>conda install -c conda-forge diffusers
</span></span></code></pre></div><h4 id="use">Use
</h4><p>Directly call the pretrained model uploaded in <code>diffusers</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> diffusers <span style="color:#f92672">import</span> DDPMPipeline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>device <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>device(<span style="color:#e6db74">&#34;cuda&#34;</span> <span style="color:#66d9ef">if</span> torch<span style="color:#f92672">.</span>cuda<span style="color:#f92672">.</span>is_available() <span style="color:#66d9ef">else</span> <span style="color:#e6db74">&#34;cpu&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load the butterfly pipeline</span>
</span></span><span style="display:flex;"><span>butterfly_pipeline <span style="color:#f92672">=</span> DDPMPipeline<span style="color:#f92672">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;johnowhitaker/ddpm-butterflies-32px&#34;</span>
</span></span><span style="display:flex;"><span>)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create 8 images</span>
</span></span><span style="display:flex;"><span>images <span style="color:#f92672">=</span> butterfly_pipeline(batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>)<span style="color:#f92672">.</span>images
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># View the result</span>
</span></span><span style="display:flex;"><span>make_grid(images)
</span></span></code></pre></div><h4 id="example">Example
</h4><h5 id="step-0-login-and-initialize-some-useful-functions">Step 0: Login and Initialize some useful functions
</h5><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Login</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> huggingface_hub <span style="color:#f92672">import</span> notebook_login
</span></span><span style="display:flex;"><span>notebook_login()
</span></span><span style="display:flex;"><span><span style="color:#75715e"># copy the token in</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn.functional <span style="color:#66d9ef">as</span> F
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> matplotlib <span style="color:#f92672">import</span> pyplot <span style="color:#66d9ef">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> PIL <span style="color:#f92672">import</span> Image
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">show_images</span>(x):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Given a batch of images x, make a grid and convert to PIL&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    x <span style="color:#f92672">=</span> x <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.5</span> <span style="color:#f92672">+</span> <span style="color:#ae81ff">0.5</span>  <span style="color:#75715e"># Map from (-1, 1) back to (0, 1)</span>
</span></span><span style="display:flex;"><span>    grid <span style="color:#f92672">=</span> torchvision<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>make_grid(x)
</span></span><span style="display:flex;"><span>    grid_im <span style="color:#f92672">=</span> grid<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>cpu()<span style="color:#f92672">.</span>permute(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">0</span>)<span style="color:#f92672">.</span>clip(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">255</span>
</span></span><span style="display:flex;"><span>    grid_im <span style="color:#f92672">=</span> Image<span style="color:#f92672">.</span>fromarray(np<span style="color:#f92672">.</span>array(grid_im)<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>uint8))
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> grid_im
</span></span></code></pre></div><h5 id="step-1-download-a-training-dataset">Step 1: Download a training dataset
</h5><p>For this example, we&rsquo;ll use a dataset of images from the Hugging Face Hub. Specifically, <a class="link" href="https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset"  target="_blank" rel="noopener"
    >this collection of 1000 butterfly pictures</a>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">import</span> torchvision
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> datasets <span style="color:#f92672">import</span> load_dataset
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> torchvision <span style="color:#f92672">import</span> transforms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load dataset from https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset</span>
</span></span><span style="display:flex;"><span>dataset <span style="color:#f92672">=</span> load_dataset(<span style="color:#e6db74">&#34;huggan/smithsonian_butterflies_subset&#34;</span>, split<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;train&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74"># Or load images from a local folder
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">dataset = load_dataset(&#34;imagefolder&#34;, data_dir=&#34;path/to/folder&#34;)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span> 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># We&#39;ll train on 32-pixel square images, but you can try larger sizes too</span>
</span></span><span style="display:flex;"><span>image_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># You can lower your batch size if you&#39;re running out of GPU memory</span>
</span></span><span style="display:flex;"><span>batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">64</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define data augmentations</span>
</span></span><span style="display:flex;"><span>preprocess <span style="color:#f92672">=</span> transforms<span style="color:#f92672">.</span>Compose(
</span></span><span style="display:flex;"><span>    [
</span></span><span style="display:flex;"><span>        transforms<span style="color:#f92672">.</span>Resize((image_size, image_size)),  <span style="color:#75715e"># Resize</span>
</span></span><span style="display:flex;"><span>        transforms<span style="color:#f92672">.</span>RandomHorizontalFlip(),  <span style="color:#75715e"># Randomly flip (data augmentation)</span>
</span></span><span style="display:flex;"><span>        transforms<span style="color:#f92672">.</span>ToTensor(),  <span style="color:#75715e"># Convert to tensor (0, 1)</span>
</span></span><span style="display:flex;"><span>        transforms<span style="color:#f92672">.</span>Normalize([<span style="color:#ae81ff">0.5</span>], [<span style="color:#ae81ff">0.5</span>]),  <span style="color:#75715e"># Map to (-1, 1)</span>
</span></span><span style="display:flex;"><span>    ]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">transform</span>(examples):
</span></span><span style="display:flex;"><span>    images <span style="color:#f92672">=</span> [preprocess(image<span style="color:#f92672">.</span>convert(<span style="color:#e6db74">&#34;RGB&#34;</span>)) <span style="color:#66d9ef">for</span> image <span style="color:#f92672">in</span> examples[<span style="color:#e6db74">&#34;image&#34;</span>]]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {<span style="color:#e6db74">&#34;images&#34;</span>: images}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dataset<span style="color:#f92672">.</span>set_transform(transform)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a dataloader from the dataset to serve up the transformed images in batches; Save the images in the dataloader</span>
</span></span><span style="display:flex;"><span>train_dataloader <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(
</span></span><span style="display:flex;"><span>    dataset, batch_size<span style="color:#f92672">=</span>batch_size, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>View the first 8 image examples in the dataset:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>xb <span style="color:#f92672">=</span> next(iter(train_dataloader))[<span style="color:#e6db74">&#34;images&#34;</span>]<span style="color:#f92672">.</span>to(device)[:<span style="color:#ae81ff">8</span>]
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;X shape:&#34;</span>, xb<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>show_images(xb)<span style="color:#f92672">.</span>resize((<span style="color:#ae81ff">8</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">64</span>), resample<span style="color:#f92672">=</span>Image<span style="color:#f92672">.</span>NEAREST)
</span></span></code></pre></div><h5 id="step-2-define-the-scheduler">Step 2: Define the Scheduler
</h5><p>Our plan for training is to take these input images and add noise to them, then feed the noisy images to the model. And during inference, we will use the model predictions to iteratively remove noise. In <code>diffusers</code>,  these processes are both handled by the <strong>scheduler</strong>.</p>
<p>The noise schedule determines how much noise is added at different timesteps.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> diffusers <span style="color:#f92672">import</span> DDPMScheduler
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define a Scheduler</span>
</span></span><span style="display:flex;"><span>noise_scheduler <span style="color:#f92672">=</span> DDPMScheduler(num_train_timesteps<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Add noise and View the process of noise-adding</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The core is add_noise()</span>
</span></span><span style="display:flex;"><span>timesteps <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">999</span>, <span style="color:#ae81ff">8</span>)<span style="color:#f92672">.</span>long()<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn_like(xb) <span style="color:#75715e"># Random a noise from standard Guassian N(0,I)</span>
</span></span><span style="display:flex;"><span>noisy_xb <span style="color:#f92672">=</span> noise_scheduler<span style="color:#f92672">.</span>add_noise(xb, noise, timesteps)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;Noisy X shape&#34;</span>, noisy_xb<span style="color:#f92672">.</span>shape)
</span></span><span style="display:flex;"><span>show_images(noisy_xb)<span style="color:#f92672">.</span>resize((<span style="color:#ae81ff">8</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">64</span>), resample<span style="color:#f92672">=</span>Image<span style="color:#f92672">.</span>NEAREST)
</span></span></code></pre></div><h5 id="step-3-define-the-model">Step 3: Define the Model
</h5><p>Most diffusion models use architectures that are some variant of a <strong>U-Net</strong> and that&rsquo;s what we&rsquo;ll use here.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> diffusers <span style="color:#f92672">import</span> UNet2DModel
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Create a model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> UNet2DModel(
</span></span><span style="display:flex;"><span>    sample_size<span style="color:#f92672">=</span>image_size,  <span style="color:#75715e"># the target image resolution</span>
</span></span><span style="display:flex;"><span>    in_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,  <span style="color:#75715e"># the number of input channels, 3 for RGB images</span>
</span></span><span style="display:flex;"><span>    out_channels<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,  <span style="color:#75715e"># the number of output channels</span>
</span></span><span style="display:flex;"><span>    layers_per_block<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,  <span style="color:#75715e"># how many ResNet layers to use per UNet block</span>
</span></span><span style="display:flex;"><span>    block_out_channels<span style="color:#f92672">=</span>(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">256</span>),  <span style="color:#75715e"># More channels -&gt; more parameters</span>
</span></span><span style="display:flex;"><span>    down_block_types<span style="color:#f92672">=</span>(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;DownBlock2D&#34;</span>,  <span style="color:#75715e"># a regular ResNet downsampling block</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;DownBlock2D&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;AttnDownBlock2D&#34;</span>,  <span style="color:#75715e"># a ResNet downsampling block with spatial self-attention</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;AttnDownBlock2D&#34;</span>,
</span></span><span style="display:flex;"><span>    ),
</span></span><span style="display:flex;"><span>    up_block_types<span style="color:#f92672">=</span>(
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;AttnUpBlock2D&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;AttnUpBlock2D&#34;</span>,  <span style="color:#75715e"># a ResNet upsampling block with spatial self-attention</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;UpBlock2D&#34;</span>,
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;UpBlock2D&#34;</span>,  <span style="color:#75715e"># a regular ResNet upsampling block</span>
</span></span><span style="display:flex;"><span>    ),
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>model<span style="color:#f92672">.</span>to(device)
</span></span></code></pre></div><p>When dealing with higher-resolution inputs you may want to use more down and up-blocks, and keep the attention layers only at the lowest resolution (bottom) layers to <strong>reduce memory usage</strong>.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Check that passing in a batch of data and some random timesteps produces an output the same shape as the input data:</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>    model_prediction <span style="color:#f92672">=</span> model(noisy_xb, timesteps)<span style="color:#f92672">.</span>sample
</span></span><span style="display:flex;"><span>model_prediction<span style="color:#f92672">.</span>shape
</span></span></code></pre></div><h5 id="step-4-training-create-a-training-loop">Step 4: Training: Create a Training Loop
</h5><p>For each batch of data, we</p>
<ul>
<li>Sample some random timesteps</li>
<li>Noise the data accordingly</li>
<li>Feed the noisy data through the model</li>
<li>Compare the model predictions with the target (i.e. the noise in this case) using mean squared error as our loss function</li>
<li>Update the model parameters via <code>loss.backward()</code> and <code>optimizer.step()</code></li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Set the noise scheduler</span>
</span></span><span style="display:flex;"><span>noise_scheduler <span style="color:#f92672">=</span> DDPMScheduler(
</span></span><span style="display:flex;"><span>    num_train_timesteps<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, beta_schedule<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;squaredcos_cap_v2&#34;</span>
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Training loop</span>
</span></span><span style="display:flex;"><span>optimizer <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>optim<span style="color:#f92672">.</span>AdamW(model<span style="color:#f92672">.</span>parameters(), lr<span style="color:#f92672">=</span><span style="color:#ae81ff">4e-4</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>losses <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Loop through the training epoch</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">30</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Loop through all data</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> step, batch <span style="color:#f92672">in</span> enumerate(train_dataloader):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Set the Scheduler</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Load the data</span>
</span></span><span style="display:flex;"><span>        clean_images <span style="color:#f92672">=</span> batch[<span style="color:#e6db74">&#34;images&#34;</span>]<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Sample noise to add to the images</span>
</span></span><span style="display:flex;"><span>        noise <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(clean_images<span style="color:#f92672">.</span>shape)<span style="color:#f92672">.</span>to(clean_images<span style="color:#f92672">.</span>device)
</span></span><span style="display:flex;"><span>        bs <span style="color:#f92672">=</span> clean_images<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Sample a random timestep for each image</span>
</span></span><span style="display:flex;"><span>        timesteps <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randint(
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">0</span>, noise_scheduler<span style="color:#f92672">.</span>num_train_timesteps, (bs,), device<span style="color:#f92672">=</span>clean_images<span style="color:#f92672">.</span>device
</span></span><span style="display:flex;"><span>        )<span style="color:#f92672">.</span>long()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Add noise to the clean images according to the noise magnitude at each timestep</span>
</span></span><span style="display:flex;"><span>        noisy_images <span style="color:#f92672">=</span> noise_scheduler<span style="color:#f92672">.</span>add_noise(clean_images, noise, timesteps)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Input the noisy_images to the model and Get the model prediction</span>
</span></span><span style="display:flex;"><span>        noise_pred <span style="color:#f92672">=</span> model(noisy_images, timesteps, return_dict<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)[<span style="color:#ae81ff">0</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Calculate the loss</span>
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>mse_loss(noise_pred, noise)
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward(loss)
</span></span><span style="display:flex;"><span>        losses<span style="color:#f92672">.</span>append(loss<span style="color:#f92672">.</span>item())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Update the model parameters with the optimizer</span>
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (epoch <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">%</span> <span style="color:#ae81ff">5</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>        loss_last_epoch <span style="color:#f92672">=</span> sum(losses[<span style="color:#f92672">-</span>len(train_dataloader) :]) <span style="color:#f92672">/</span> len(train_dataloader)
</span></span><span style="display:flex;"><span>        print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Epoch:</span><span style="color:#e6db74">{</span>epoch<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span><span style="color:#e6db74">}</span><span style="color:#e6db74">, loss: </span><span style="color:#e6db74">{</span>loss_last_epoch<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Plot the loss</span>
</span></span><span style="display:flex;"><span>fig, axs <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>subplots(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">4</span>))
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">0</span>]<span style="color:#f92672">.</span>plot(losses)
</span></span><span style="display:flex;"><span>axs[<span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>plot(np<span style="color:#f92672">.</span>log(losses))
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><h5 id="step-5-generate-images">Step 5: Generate Images
</h5><ul>
<li>Method 1: Create a pipeline</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> diffusers <span style="color:#f92672">import</span> DDPMPipeline
</span></span><span style="display:flex;"><span>image_pipe <span style="color:#f92672">=</span> DDPMPipeline(unet<span style="color:#f92672">=</span>model, scheduler<span style="color:#f92672">=</span>noise_scheduler)
</span></span><span style="display:flex;"><span>pipeline_output <span style="color:#f92672">=</span> image_pipe()
</span></span><span style="display:flex;"><span>pipeline_output<span style="color:#f92672">.</span>images[<span style="color:#ae81ff">0</span>]
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># save a pipeline to a local folder like so:</span>
</span></span><span style="display:flex;"><span>image_pipe<span style="color:#f92672">.</span>save_pretrained(<span style="color:#e6db74">&#34;my_pipeline&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#75715e"># Inspecting the folder contents:</span>
</span></span><span style="display:flex;"><span>ls my_pipeline/
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Output: model_index.json  scheduler  unet</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># The `scheduler` and `unet` subfolders contain everything needed to re-create those components. For example, inside the `unet` folder you&#39;ll find the model weights (`diffusion_pytorch_model.bin`) alongside a config file which specifies the UNet architecture. </span>
</span></span></code></pre></div><ul>
<li>Method 2: Writing a Sampling Loop</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># Random starting point (8 random images):</span>
</span></span><span style="display:flex;"><span>sample <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>randn(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>)<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># A specific process of denoising and generating the images</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, t <span style="color:#f92672">in</span> enumerate(noise_scheduler<span style="color:#f92672">.</span>timesteps):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Get model pred</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">with</span> torch<span style="color:#f92672">.</span>no_grad():
</span></span><span style="display:flex;"><span>        residual <span style="color:#f92672">=</span> model(sample, t)<span style="color:#f92672">.</span>sample
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Update sample with step</span>
</span></span><span style="display:flex;"><span>    sample <span style="color:#f92672">=</span> noise_scheduler<span style="color:#f92672">.</span>step(residual, t, sample)<span style="color:#f92672">.</span>prev_sample
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>show_images(sample)
</span></span></code></pre></div>
</section>


    <footer class="article-footer">
    

    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>All Rights Reserved</span>
    </section>
    </footer>


    
</article>

    

    

     
    
        
    <div class="disqus-container">
    
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (typeof DISQUS == 'object') {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2025 Serendipity_Blog
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.30.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
