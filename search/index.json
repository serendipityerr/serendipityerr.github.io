[{"content":"Update Hugo To Hugo-extended\rWhy not Hugo but Hugo-extended?\nSome themes use SCSS and TypeScript, that\u0026rsquo;s why Hugo extended version is required. If you are using a non-extended Hugo installation, you will get the following error:\n1 Error: Error building site: TOCSS: failed to transform \u0026#34;scss/style.scss\u0026#34; (text/x-scss): this feature is not available in your current Hugo version Download\rGet to https://github.com/gohugoio/hugo/releases and download e.g. hugo_extended_0.143.1_windows-amd64.zip. Or just winget in the cmd:\n1 winget install Hugo.Hugo.Extended Extract\rExtract the zip in the suitable location:\n1 2 3 4 5 PS D:\\\u0026gt; tree .\\hugo_extended /f /a D:\\HUGO_EXTENDED hugo.exe LICENSE README.md Verify\rVerify the installation:\n1 2 PS D:\\\u0026gt; D:/hugo_extended/hugo version hugo v0.143.1-0270364a347b2ece97e0321782b21904db515ecc+extended windows/amd64 BuildDate=2025-02-04T08:57:38Z VendorInfo=gohugoio After that, the origin hugo D:/hugo_dir can be deprecated and all the commands e.g. D:/hugo_dir/hugo are changed to D:/hugo_extended/hugo. ✅\n","date":"2025-02-17T18:57:48+08:00","image":"https://example.com/hugo-logo-wide.png","permalink":"https://example.com/p/update-hugo-to-hugo-extended/","title":"Update Hugo To Hugo-extended"},{"content":"Solve the problem of export latex code in Markdown as pdf\rFirst, download VSCode and the Markdown All in One extension.\nSecond, Ctrl + Shift + P and input Print current document to HTML.\nA html file is already created and with well-displayed latex equation.\n","date":"2024-12-09T18:50:29+08:00","permalink":"https://example.com/p/2024.12.09/","title":"2024.12.09"},{"content":"Python permanently modifies pip mirror source\rIf change the mirror source to Tsinghua Mirror Source, type the following code in the Terminal:\n1 pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple Install Cuda In Virtual Environment of Conda\rFirst Run the code in the Terminal to determine the version of Cuda:\n1 nvidia-smi CUDA Version is the version of cuda that the computer can support, so the version of cuda we want to install needs to be \u0026lt;= CUDA Version (backward compatible)\nRun the code in order in the Terminal:\n1 2 3 4 5 6 7 8 9 conda create -n env_name python=3.10 conda activate env_name # conda search cudatoolkit --info conda install cudatoolkit=11.8.0 conda install cudnn # pytorch official website: https://pytorch.org/get-started/locally/ conda install pytorch torchvision torchaudio pytorch-cuda=12.4 -c pytorch -c nvidia After installing the Cuda, Run the code in order in the Terminal and verify that Cuda was installed successfully:\n1 2 3 4 conda activate env_name python import torch print(torch.cuda.is_available()) ","date":"2024-10-18T15:28:15+08:00","permalink":"https://example.com/p/2024.10.18/","title":"2024.10.18"},{"content":"Image Pre-processing/Transformation\r1 2 3 4 5 6 7 8 9 10 11 train_tfm = transforms.Compose([ # Resize the images into the fixed size transforms.Resize((128, 128)), \u0026#39;\u0026#39;\u0026#39; Do some Image Enhancement \u0026#39;\u0026#39;\u0026#39; # ToTensor() should be the last transformation transforms.ToTensor(), ]) Geometric Transformations\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Rotation transform_rotate = transforms.RandomRotation(degrees=30) # Translation transform_translate = transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)) # Flipping transform_flip = transforms.RandomHorizontalFlip(p=0.5) # Scaling transform_scale = transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)) # Shearing transform_shear = transforms.RandomAffine(degrees=0, shear=20) Color Transformations\r1 2 3 4 5 6 7 8 9 10 11 # Brightness Adjustment transform_brightness = transforms.ColorJitter(brightness=0.5) # Contrast Adjustment transform_contrast = transforms.ColorJitter(contrast=0.5) # Satuation Adjustment transform_saturation = transforms.ColorJitter(saturation=0.5) # Hue Adjustment transform_hue = transforms.ColorJitter(hue=0.2) Cropping and Padding\r1 2 3 4 5 # Random Cropping transform_random_crop = transforms.RandomCrop(size=224) # Padding transform_padding = transforms.Pad(padding=4) Image Enhancement\r1 2 # Random Erasing transform_random_erasing = transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0) ","date":"2024-08-28T14:14:07+08:00","permalink":"https://example.com/p/2024.08.28/","title":"2024.08.28"},{"content":"Learning Diffusers\rInstallation\rWith pip\n1 pip install --upgrade diffusers[torch] With conda\n1 conda install -c conda-forge diffusers Use\rDirectly call the pretrained model uploaded in diffusers:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torch from diffusers import DDPMPipeline device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) # Load the butterfly pipeline butterfly_pipeline = DDPMPipeline.from_pretrained( \u0026#34;johnowhitaker/ddpm-butterflies-32px\u0026#34; ).to(device) # Create 8 images images = butterfly_pipeline(batch_size=8).images # View the result make_grid(images) Example\rStep 0: Login and Initialize some useful functions\r1 2 3 4 # Login from huggingface_hub import notebook_login notebook_login() # copy the token in 1 2 3 4 5 6 7 8 9 10 11 12 13 import numpy as np import torch import torch.nn.functional as F from matplotlib import pyplot as plt from PIL import Image def show_images(x): \u0026#34;\u0026#34;\u0026#34;Given a batch of images x, make a grid and convert to PIL\u0026#34;\u0026#34;\u0026#34; x = x * 0.5 + 0.5 # Map from (-1, 1) back to (0, 1) grid = torchvision.utils.make_grid(x) grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255 grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8)) return grid_im Step 1: Download a training dataset\rFor this example, we\u0026rsquo;ll use a dataset of images from the Hugging Face Hub. Specifically, this collection of 1000 butterfly pictures.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 import torchvision from datasets import load_dataset from torchvision import transforms # Load dataset from https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset dataset = load_dataset(\u0026#34;huggan/smithsonian_butterflies_subset\u0026#34;, split=\u0026#34;train\u0026#34;) \u0026#39;\u0026#39;\u0026#39; # Or load images from a local folder dataset = load_dataset(\u0026#34;imagefolder\u0026#34;, data_dir=\u0026#34;path/to/folder\u0026#34;) \u0026#39;\u0026#39;\u0026#39; # We\u0026#39;ll train on 32-pixel square images, but you can try larger sizes too image_size = 32 # You can lower your batch size if you\u0026#39;re running out of GPU memory batch_size = 64 # Define data augmentations preprocess = transforms.Compose( [ transforms.Resize((image_size, image_size)), # Resize transforms.RandomHorizontalFlip(), # Randomly flip (data augmentation) transforms.ToTensor(), # Convert to tensor (0, 1) transforms.Normalize([0.5], [0.5]), # Map to (-1, 1) ] ) def transform(examples): images = [preprocess(image.convert(\u0026#34;RGB\u0026#34;)) for image in examples[\u0026#34;image\u0026#34;]] return {\u0026#34;images\u0026#34;: images} dataset.set_transform(transform) # Create a dataloader from the dataset to serve up the transformed images in batches; Save the images in the dataloader train_dataloader = torch.utils.data.DataLoader( dataset, batch_size=batch_size, shuffle=True ) View the first 8 image examples in the dataset:\n1 2 3 xb = next(iter(train_dataloader))[\u0026#34;images\u0026#34;].to(device)[:8] print(\u0026#34;X shape:\u0026#34;, xb.shape) show_images(xb).resize((8 * 64, 64), resample=Image.NEAREST) Step 2: Define the Scheduler\rOur plan for training is to take these input images and add noise to them, then feed the noisy images to the model. And during inference, we will use the model predictions to iteratively remove noise. In diffusers, these processes are both handled by the scheduler.\nThe noise schedule determines how much noise is added at different timesteps.\n1 2 3 from diffusers import DDPMScheduler # Define a Scheduler noise_scheduler = DDPMScheduler(num_train_timesteps=1000) 1 2 3 4 5 6 7 # Add noise and View the process of noise-adding # The core is add_noise() timesteps = torch.linspace(0, 999, 8).long().to(device) noise = torch.randn_like(xb) # Random a noise from standard Guassian N(0,I) noisy_xb = noise_scheduler.add_noise(xb, noise, timesteps) print(\u0026#34;Noisy X shape\u0026#34;, noisy_xb.shape) show_images(noisy_xb).resize((8 * 64, 64), resample=Image.NEAREST) Step 3: Define the Model\rMost diffusion models use architectures that are some variant of a U-Net and that\u0026rsquo;s what we\u0026rsquo;ll use here.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from diffusers import UNet2DModel # Create a model model = UNet2DModel( sample_size=image_size, # the target image resolution in_channels=3, # the number of input channels, 3 for RGB images out_channels=3, # the number of output channels layers_per_block=2, # how many ResNet layers to use per UNet block block_out_channels=(64, 128, 128, 256), # More channels -\u0026gt; more parameters down_block_types=( \u0026#34;DownBlock2D\u0026#34;, # a regular ResNet downsampling block \u0026#34;DownBlock2D\u0026#34;, \u0026#34;AttnDownBlock2D\u0026#34;, # a ResNet downsampling block with spatial self-attention \u0026#34;AttnDownBlock2D\u0026#34;, ), up_block_types=( \u0026#34;AttnUpBlock2D\u0026#34;, \u0026#34;AttnUpBlock2D\u0026#34;, # a ResNet upsampling block with spatial self-attention \u0026#34;UpBlock2D\u0026#34;, \u0026#34;UpBlock2D\u0026#34;, # a regular ResNet upsampling block ), ) model.to(device) When dealing with higher-resolution inputs you may want to use more down and up-blocks, and keep the attention layers only at the lowest resolution (bottom) layers to reduce memory usage.\n1 2 3 4 # Check that passing in a batch of data and some random timesteps produces an output the same shape as the input data: with torch.no_grad(): model_prediction = model(noisy_xb, timesteps).sample model_prediction.shape Step 4: Training: Create a Training Loop\rFor each batch of data, we\nSample some random timesteps Noise the data accordingly Feed the noisy data through the model Compare the model predictions with the target (i.e. the noise in this case) using mean squared error as our loss function Update the model parameters via loss.backward() and optimizer.step() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # Set the noise scheduler noise_scheduler = DDPMScheduler( num_train_timesteps=1000, beta_schedule=\u0026#34;squaredcos_cap_v2\u0026#34; ) # Training loop optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4) losses = [] # Loop through the training epoch for epoch in range(30): # Loop through all data for step, batch in enumerate(train_dataloader): # Set the Scheduler # Load the data clean_images = batch[\u0026#34;images\u0026#34;].to(device) # Sample noise to add to the images noise = torch.randn(clean_images.shape).to(clean_images.device) bs = clean_images.shape[0] # Sample a random timestep for each image timesteps = torch.randint( 0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device ).long() # Add noise to the clean images according to the noise magnitude at each timestep noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps) # Input the noisy_images to the model and Get the model prediction noise_pred = model(noisy_images, timesteps, return_dict=False)[0] # Calculate the loss loss = F.mse_loss(noise_pred, noise) loss.backward(loss) losses.append(loss.item()) # Update the model parameters with the optimizer optimizer.step() optimizer.zero_grad() if (epoch + 1) % 5 == 0: loss_last_epoch = sum(losses[-len(train_dataloader) :]) / len(train_dataloader) print(f\u0026#34;Epoch:{epoch+1}, loss: {loss_last_epoch}\u0026#34;) 1 2 3 4 5 # Plot the loss fig, axs = plt.subplots(1, 2, figsize=(12, 4)) axs[0].plot(losses) axs[1].plot(np.log(losses)) plt.show() Step 5: Generate Images\rMethod 1: Create a pipeline 1 2 3 4 from diffusers import DDPMPipeline image_pipe = DDPMPipeline(unet=model, scheduler=noise_scheduler) pipeline_output = image_pipe() pipeline_output.images[0] 1 2 # save a pipeline to a local folder like so: image_pipe.save_pretrained(\u0026#34;my_pipeline\u0026#34;) 1 2 3 4 # Inspecting the folder contents: ls my_pipeline/ # Output: model_index.json scheduler unet # The `scheduler` and `unet` subfolders contain everything needed to re-create those components. For example, inside the `unet` folder you\u0026#39;ll find the model weights (`diffusion_pytorch_model.bin`) alongside a config file which specifies the UNet architecture. Method 2: Writing a Sampling Loop 1 2 3 4 5 6 7 8 9 10 11 12 13 # Random starting point (8 random images): sample = torch.randn(8, 3, 32, 32).to(device) # A specific process of denoising and generating the images for i, t in enumerate(noise_scheduler.timesteps): # Get model pred with torch.no_grad(): residual = model(sample, t).sample # Update sample with step sample = noise_scheduler.step(residual, t, sample).prev_sample show_images(sample) ","date":"2024-08-11T12:09:12+08:00","permalink":"https://example.com/p/2024.08.11/","title":"2024.08.11"},{"content":"\r","date":"2024-08-02T11:22:51+08:00","permalink":"https://example.com/p/2024.08.02/","title":"2024.08.02"},{"content":"Difference Between PCA and AutoEncoder\rPCA\rSuppose there are m n-dimensional data, $ X_{n \\times m} = [x_1, x_2,\u0026hellip;, x_m]$, where each $x$ is an n-dimensional column vector.\nReduce Dimensionality\rDecentralize the data: $x_{i} = x_{i} - \\frac{1}{m} \\sum_{j=1}^{m} x_{j}$ and update $X$ Calculate the Covariance matrix: $C = \\frac{1}{m} XX^T$ Take eigenvalue decomposition of the Covariance matrix $C$ and get the eigenvector matrix (the eigenvector is arranged in columns from the related largest to smallest eigenvalues). Take the first $k$ columns to form the matrix $P_{n \\times k}$ Project the original data into the $P$ coordinate system to get the dimensionality reduced data: $Y_{k \\times m} = P_{n \\times k}^T \\times X_{n \\times m}$, which is a linear transformation. The dimension of the data after PCA is changed compared to the origin data. Data Reconstruction\rPCA is lossy, that is, the compressed data does not maintain all the information of the original data, so the compressed data can not be restored back to the original high-dimensional data, but the restored data can be regarded as an approximation of the original data: $X_{n \\times m}^{\u0026rsquo;} = P_{n \\times k} Y_{k \\times m}$\nAutoEncoder\rEncoder\rThe original data $X$ is input, and then compressed according to the network model, the original high dimensional data $X$ is compressed into low dimensional data C, and these low dimensional data is usually customarily referred to as latent vector, the original data after the activation function operation of the nonlinear hidden layer, the original data will be transformed into a low dimensional space, this space is considered to be the high-feature space. After the original data is operated by the activation function of the nonlinear hidden layer, the original data will be transformed to a low-dimensional space, which is considered as the high-feature space. AutoEncoder is a non-linear transformation. The dimension of the data after Encoder is the same as the origin data.\nDecoder\rConvert the original implicit layer data back into the original data space.\nHow to design the network\rFor simple datasets such as MNIST, a network with 1-2 hidden layers is usually sufficient. However, a network with 3 or more hidden layers can capture more complex features, but can also lead to overfitting.\nAs for MNIST, the number of nodes in the hidden layer should decrease layer by layer, usually the number of nodes in the last layer of the encoder, i.e., the dimension of the potential space, can be chosen from 32 to 128.\nCode Example:\rAutoEncoder on MNIST dataset:\n1 2 3 4 5 6 7 8 self.encoder = nn.Sequential( nn.Linear(28 * 28, 128), nn.ReLU(True), nn.Linear(128, 64), nn.ReLU(True), nn.Linear(64, 32) ) # Design of the network can be changed 1 2 3 4 5 6 7 8 9 self.decoder = nn.Sequential( nn.Linear(32, 64), nn.ReLU(True), nn.Linear(64, 128), nn.ReLU(True), nn.Linear(128, 28 * 28), nn.Tanh() ) # Design of the network can be changed ","date":"2024-07-20T14:13:42+08:00","permalink":"https://example.com/p/2024.07.20/","title":"2024.07.20"},{"content":"Study on Score-Based Generative Modeling\r(●\u0026rsquo;◡\u0026rsquo;●)\nAbout Score-Based Generative Modeling: the following article has given a clear explanation: Diffusion Model: SDE\nA simple conclusion of the Score-Based Generative Modeling through Stochastic Differential Equations: Generalization of discrete models such as DDPM to continuous SDE forms, through predictor-corrector samplers to correct the solution of a numerical SDE solver.\n","date":"2024-07-18T23:45:09+08:00","permalink":"https://example.com/p/2024.07.18/","title":"2024.07.18"},{"content":"Study how to build a github repository\rԅ(¯﹃¯ԅ)\nFirst create a github repository and set the repository name.\nRun the code in order in the PowerShell under the target directory:\n1 2 3 4 5 6 cd dir_name git init git add . git commit -m \u0026#34;first commit\u0026#34; git remote add origin https://github.com/.... git push origin main Study how to build a blog\rNew a post\r( •̀ .̫ •́ )✧\nRun the code in order in the PowerShell:\n1 2 cd D:/html D:/hugo_extended/hugo new post/fileName.md Push to Github\r(｡･ω･｡)ﾉ♡\nRun the code in order in the PowerShell:\n1 2 3 4 5 D:/hugo_extended/hugo cd public git add . git commit -m \u0026#34;test\u0026#34; git push origin main ","date":"2024-07-16T14:16:38+08:00","permalink":"https://example.com/p/2024.07.16/","title":"2024.07.16"},{"content":"Hello Serendpity\u0026rsquo;s Blog\rThis is Serendpity\u0026rsquo;s first blog post. Love you all! ( •̀ ω •́ )y\n","date":"2024-07-16T13:40:30+08:00","permalink":"https://example.com/p/my-first-blog/","title":"My First Blog"}]